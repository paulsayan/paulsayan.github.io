<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sayan Paul </title> <meta name="author" content="Sayan Paul"> <meta name="description" content=""> <meta name="keywords" content="robotics, computer vision, machine learning, artificial intelligence, research"> <meta property="og:site_name" content="Sayan Paul"> <meta property="og:type" content="website"> <meta property="og:title" content="Sayan Paul | About"> <meta property="og:url" content="https://sayanpaul.com/"> <meta property="og:description" content=""> <meta property="og:image" content="opengraph_image_2020204.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="About"> <meta name="twitter:description" content=""> <meta name="twitter:image" content="opengraph_image_2020204.png"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sayan Paul"
        },
        "url": "https://sayanpaul.com/",
        "@type": "WebSite",
        "description": "",
        "headline": "About",
        
        "sameAs": ["https://scholar.google.com/citations?user=-XgsQ64AAAAJ","https://www.linkedin.com/in/sayan-paul","https://orcid.org/0000-0001-9885-233X","https://twitter.com/sayanpaul_"],
        
        "name": "Sayan Paul",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon1.png?b4137608163301117b558803a10ac5f1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sayanpaul.com/"> <script src="/assets/js/theme.js?981d193ff68e8eae6940544ac854727c"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="https://scholar.google.com/citations?user=-XgsQ64AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.linkedin.com/in/sayan-paul" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0001-9885-233X" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://twitter.com/sayanpaul_" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/research-papers/">Research Papers</a> <a class="dropdown-item " href="/patents/">Patents</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/awards-and-recognition/">Awards &amp; Recognition </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Sayan Paul </h1> <p class="desc"><a href="#">Helping Robots understand our World so they can have our back.</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sayanpaul_profile_pic_jan2020-480.webp 480w,/assets/img/sayanpaul_profile_pic_jan2020-800.webp 800w,/assets/img/sayanpaul_profile_pic_jan2020-1400.webp 1400w," type="image/webp" sizes="(min-width: 1024px) 298.2px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/sayanpaul_profile_pic_jan2020.jpg?1a467408af906992330223f3b22a8b64" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="sayanpaul_profile_pic_jan2020.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>Hi! I‚Äôm a curious geek who enjoys exploring emerging technologies üîç to research &amp; develop novel solutions to real-world technical problems, with a long-term goal of advancing intelligent robotics ü§ñ for the benefit of society üåç.</p> <p>Currently I‚Äôm associated with Visual Computing &amp; Embodied AI Research Lab (previously part of Robotics and Autonomous Systems Research Lab) of <a href="https://www.tcs.com/what-we-do/research" target="_blank" rel="external nofollow noopener">TCS Research</a> as a Senior Research Engineer. My interests lie at the intersection of robotics, computer vision and machine learning (DL, RL, etc). In recent years, I have tinkered with robot perception, 3D vision, visual SLAM, scene understanding, robot navigation, planning, autonomous exploration, human-robot interaction, and telepresence robots.</p> <p>Prior to that, I led <a href="https://in.linkedin.com/company/robonixnsec" target="_blank" rel="external nofollow noopener">Robonix NSEC</a>, the official Robotics Club of our college where I coordinated various robotics events &amp; activities and mentored other students in Autonomous Robotics by conducting sessions, forums &amp; bootcamps.</p> <p>I love connecting with people to learn about their unique journeys and goals. I believe in lifelong learning, sharing, and collaboration; and I enjoy mentoring and guiding others in areas where I have expertise.</p> <p>Outside of work, I‚Äôm a photography enthusiast üì∏, a DIY electronics hobbyist üõ†Ô∏è, and I enjoy leisure hiking &amp; traveling in the mountains üèîÔ∏è.</p> </div> <hr> <h3> <a href="/publications/" style="color: inherit">Selected Publications</a> </h3> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/pub_mpvo2024_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pub_mpvo2024_preview.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="MPVO2024" class="col-sm-8"> <div class="title">MPVO: Motion-Prior Based Visual Odometry for¬†PointGoal Navigation</div> <div class="author"> <em>Sayan Paul</em>, Ruddra dev Roychoudhury, and Brojeshwar Bhowmick </div> <div class="periodical"> <em>In 18th European Conference on Computer Vision (ECCV), 50SFM Workshop</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-91569-7_9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2411.04796" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/pub_doro2022_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pub_doro2022_preview.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="doro2022" class="col-sm-8"> <div class="title">DoRO: Disambiguation of Referred Object for Embodied Agents</div> <div class="author"> Pradip Pramanick, Chayan Sarkar, <em>Sayan Paul</em>, Ruddra dev Roychoudhury, and Brojeshwar Bhowmick </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/LRA.2022.3195198" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2207.14205" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Robotic task instructions often involve a referred object that the robot must locate (ground) within the environment. While task intent understanding is an essential part of natural language understanding, less effort is made to resolve ambiguity that may arise while grounding the task. Existing works use vision-based task grounding and ambiguity detection, suitable for a fixed view and a static robot. However, the problem magnifies for a mobile robot, where the ideal view is not known beforehand. Moreover, a single view may not be sufficient to locate all the object instances in the given area, which leads to inaccurate ambiguity detection. Human intervention is helpful only if the robot can convey the kind of ambiguity it is facing. In this article, we present DoRO (Disambiguation of Referred Object), a system that can help an embodied agent to disambiguate the referred object by raising a suitable query whenever required. Given an area where the intended object is, DoRO finds all the instances of the object by aggregating observations from multiple views while exploring &amp; scanning the area. It then raises a suitable query using the information from the grounded object instances. Experiments conducted with the AI2Thor simulator show that DoRO not only detects the ambiguity more accurately but also raises verbose queries with more accurate information from the visual-language grounding.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Sayan Paul. Last updated: February 04, 2026. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="/assets/libs/mathjax/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="/assets/libs/polyfill/polyfill.min.js" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>